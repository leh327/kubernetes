https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
https://opensource.com/article/20/6/kubernetes-raspberry-pi
https://www.weave.works/blog/running-highly-available-clusters-with-kubeadm

https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/
https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/

https://icicimov.github.io/blog/kubernetes/Kubernetes-cluster-step-by-step-Part5/

# haproxy lb 
kubectl apply -f https://raw.githubusercontent.com/haproxytech/kubernetes-ingress/master/deploy/haproxy-ingress.yaml

# install kubernetes packages
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF
apt update
apt install -y kubelet kubeadm kubectl

apt install -y containerd
apt install -y docker.io
docker info
cat > /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF
sed -i '$ s/$/ cgroup_enable=cpuset cgroup_enable=memory cgroup_memory=1 swapaccount=1/' /boot/firmware/cmdline.txt

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
$ sudo sysctl --system

# what need to be setup
1. Install helm package manager repos

1. create lb to use in kubeadm control-plane-endpoint value:: https://github.com/kubernetes/kubeadm/blob/master/docs/ha-considerations.md#options-for-software-load-balancing

2. Use docker container registry - create an account at dockerhub
   Or create local registry

3. Use stack etcd in each of the control plane or set up a High Availability etcd cluster with kubeadm
#  Use stack etcd in each of the control plane

# set up a High Availability etcd cluster with kubeadm
# create kubelet service for etcd
cat << EOF > /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf
[Service]
ExecStart=
#  Replace "systemd" with the cgroup driver of your container runtime. The default value in the kubelet is "cgroupfs".
ExecStart=/usr/bin/kubelet --address=127.0.0.1 --pod-manifest-path=/etc/kubernetes/manifests --cgroup-driver=systemd
Restart=always
EOF

systemctl daemon-reload
systemctl restart kubelet

# create configuration files for kubeadm
  run gen_config_file.sh

# Generate selfsign certificate authority on first host - HOST0
kubeadm init phase certs etcd-ca
   
4. Install control planes
   TOKEN=$(kubeadm token generate)
   kubeadm init --token=$TOKEN --kubernetes-version=v1.19.2 --pod-network-cidr=10.224.0.0/16 --control-plane-endpoint=192.168.4.10
   # apply CNI provider
	kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
	or
	kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
4. join worker nodes into kubernetes
5. Install ha-proxy load balancer: https://www.weave.works/blog/running-highly-available-clusters-with-kubeadm
6. install ingress
6. Create Service


 kubectl label node k8s-worker03 node-role.kubernetes.io/worker=worker
 kubectl label node k8s-worker1 node-role.kubernetes.io/worker=worker
 kubectl label node k8s-worker4 node-role.kubernetes.io/worker=worker
 kubectl label node k8s-worker6 node-role.kubernetes.io/worker=worker

